{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8459f4e9e52473b4fe8824724a87798e1946e02"
   },
   "source": [
    "# Modeling , Param Tuning, Evulating, Explaining & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "_uuid": "f159da396824602d8ae73fc5cb08f84987ce91f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pylab import has clobbered these variables: ['identity', 'datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "from datetime import datetime\n",
    "import pickle \n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,log_loss,roc_curve, auc\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import shap\n",
    "import json\n",
    "import imblearn\n",
    "\n",
    "import DATA_PROCESSING as process\n",
    "%pylab inline\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 1000000\n",
    "USE_SMOTE = True\n",
    "CALC_PARAMS = True\n",
    "dev_mode = True\n",
    "config_file = \"configuration_01_15_2022__08_18_sample_1000000.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_cum =['app_cat','manufacturer','device_model','device_version','user_isp']\n",
    "columns_dummies=['banner_pos', 'Day_of_Week','state','manufacturer','Month','hour','user_isp','app_cat','device_model','device_version']\n",
    "features = ['state', 'user_isp', 'app_cat', 'banner_pos', 'manufacturer', 'device_model', 'device_version', 'device_height', 'device_width', 'device_diag', 'Day_of_Week', 'Month', 'hour']\n",
    "cat_features = ['state', 'user_isp', 'app_cat', 'banner_pos', 'manufacturer', 'device_model', 'device_version', 'Day_of_Week', 'Month', 'hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"train_set.pickle\").sample(n=SAMPLE)\n",
    "df.loc[:,\"clicked\"] = df.loc[:,\"clicked\"].map({True:1, False:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing, Feature engineering,Cardinality reductions , and dummy coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(df, one_hot_encoding=True, allowed_levels_dict = None):\n",
    "    df= df.drop(['op_id', 'resolution','app_id'], axis=1)\n",
    "    # if  a dict of categorical columns and theyr allowed levels is given - replace levels not in the list as \"<col_name>_other\"\n",
    "    if allowed_levels_dict:\n",
    "        for col_name, categories_list in allowed_levels_dict.items():\n",
    "            df.loc[:, col_name] = df[col_name].map(lambda x: x if x in categories_list else f\"{col_name}_other\")\n",
    "    df=process.Missing_values(df)\n",
    "    df=process.Feature_engineering(df)\n",
    "    if not allowed_levels_dict:\n",
    "        df, cat_dict = process.cumulatively_categorise(df,columns_cum)\n",
    "    if one_hot_encoding:\n",
    "        df=process.get_dummies_fun(df,columns_dummies)\n",
    "    return df,allowed_levels_dict if allowed_levels_dict else cat_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of missing values before imputing missing data: \n",
      "user_isp            15\n",
      "manufacturer    306036\n",
      "device_model    306003\n",
      "dtype: int64\n",
      "summary of missing values after imputing missing data based on device version\n",
      "manufacturer    1283\n",
      "device_model    1283\n",
      "dtype: int64\n",
      "summary of missing values after imputing missing data by assigning a separate category\n",
      "Series([], dtype: int64)\n",
      "Wall time: 26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 245)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df, allowed_levels_dict = preprocessing_pipeline(df, one_hot_encoding=USE_SMOTE)\n",
    "MODEL_COLS = list(df.columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring the Target Encoder\n",
    "\n",
    "for classification tasks Target encoding is a novel and usefull method for converting high-cardinality categorical variable to a continuous value by encoding for each level the propoprion of rows in which Y=1\\\n",
    "i.e. for click prediction it encodes the ctr for each level over the entire dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder = ce.TargetEncoder(cols=cat_features)\n",
    "encoded = encoder.fit_transform(df[features], df['clicked'])\n",
    "encoded.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at Class (Im)Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clicked\n",
       "0    932181\n",
       "1     67819\n",
       "Name: clicked, dtype: int64"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('clicked').clicked.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting the data test to 3 parts:\n",
    "1. training: 80%\n",
    "2. validation: 10% - for param running \n",
    "3. test set: 10% a final test set to evaluate the perfromance of the final model, only after all tunning has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, other = train_test_split(df, test_size=0.2, random_state=42)\n",
    "dev_val, dev_test = train_test_split(other, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800000, 245), (100000, 245), (100000, 245))"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, dev_val.shape,  dev_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE oversampling - on the training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appply_smote(df):\n",
    "    cat_features =  [c for c in df.columns if 'device_' not in c and c != 'clicked']\n",
    "    features =  [c for c in df.columns if c != 'clicked']\n",
    "    \n",
    "    kw_args={\n",
    "                'y':df['clicked'],\n",
    "                'X_cols' :list(df[features].columns),\n",
    "                'y_cols' :list(df[['clicked']].columns)\n",
    "            }\n",
    "    smote_transformer = FunctionTransformer(process.Smote_alg,kw_args=kw_args,validate=True)\n",
    "\n",
    "    smote_pipe = Pipeline([('smote', smote_transformer)])\n",
    "    new_df, new_y = smote_pipe.fit_transform(df[features])\n",
    "\n",
    "    #print ('Before', df[features].shape, df['clicked'].shape, 'After', new_df.shape, new_y.shape)\n",
    "    \n",
    "    new_df.loc[:, 'clicked'] = new_y['clicked']\n",
    "    df = new_df[MODEL_COLS]\n",
    "    cat_features = []\n",
    "    return df, features, cat_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org X.shape (800000, 244) len(X_cols) 244\n",
      "new_X.shape (1491268, 244)\n"
     ]
    }
   ],
   "source": [
    "#TODO add the explanation here\n",
    "if USE_SMOTE:\n",
    "    train_df, features, cat_features = appply_smote(train_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[features]\n",
    "y_train = train_df['clicked']\n",
    "X_val = dev_val[features]\n",
    "y_val = dev_val['clicked']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(model):\n",
    "    d = configs.loc[model].params\n",
    "    params = json.loads(d)\n",
    "    print (params)\n",
    "    return (params)\n",
    "configs = pd.read_csv(config_file).set_index('index')\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(fpr, tpr):\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=\"ROC curve (area = %0.3f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_calibration(y, y_hat):\n",
    "    # reliability diagram\n",
    "    fop, mpv = calibration_curve(y, y_hat, n_bins=10, normalize=True)\n",
    "    # plot perfectly calibrated\n",
    "    pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot model reliability\n",
    "    pyplot.plot(mpv, fop, marker='.')\n",
    "    plt.xlabel(\"Mean predicted probabilities\")\n",
    "    plt.ylabel(\"True Probabilities\")\n",
    "    plt.title(\"Calibration Curve\")\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold_by_accuracy(y, y_hat, plot=False):\n",
    "    threshold = []\n",
    "    accuracy = []\n",
    "    for p in np.sort(np.unique(y_hat))[-50:]:\n",
    "        threshold.append(p)\n",
    "        y_pred = (y_hat >= p).astype(int)\n",
    "        curr_accuracy = accuracy_score(y,y_pred)\n",
    "        accuracy.append(curr_accuracy)\n",
    "    best_threshold = threshold[np.argmax(np.array(accuracy), axis=0)]\n",
    "    best_accuracy = max(accuracy)\n",
    "    print (\"Best Threshold:\" ,best_threshold, \", with accuracy = \", best_accuracy)\n",
    "    if plot:\n",
    "        plt.scatter(threshold,accuracy)\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()\n",
    "    return best_accuracy, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_params_random_forest(grid, X_train, y_train, X_val, y_val):\n",
    "    best_metric = 0\n",
    "    worst_metric = 1\n",
    "    for g in ParameterGrid(grid):\n",
    "        print(g)\n",
    "        rf_pipe = make_rf_pipe_line(None, params = g, target_encoding = (USE_SMOTE == False))\n",
    "        rf_pipe.fit(X_train,y_train)\n",
    "        y_hat = rf_pipe.predict_proba(X_val)[:,1]\n",
    "        curr_metric, best_threshold = find_threshold_by_accuracy(y_val, y_hat, plot=False)\n",
    "\n",
    "        # save if best\n",
    "        if curr_metric > best_metric:\n",
    "            best_metric = curr_metric\n",
    "            best_grid = g\n",
    "            \n",
    "        if curr_metric < worst_metric:\n",
    "            worst_metric = curr_metric\n",
    "            worst_grid = g\n",
    "    print (\"Final Result\")\n",
    "    print (\"best metric\", best_metric, \", Grid:\", best_grid)\n",
    "    print (\"worst metric\", worst_metric, \", Grid:\", best_grid)\n",
    "    return best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_pipeline(pipe, X, y, params):\n",
    "    y_hat = pipe.predict_proba(X)[:,1]\n",
    "    assert y.shape ==  y_hat.shape\n",
    "    best_accuracy, best_threshold = find_threshold_by_accuracy(y_val, y_hat, plot=False)\n",
    "    \n",
    "    y_pred = (y_hat >= best_threshold).astype(int)\n",
    "\n",
    " \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_hat, pos_label=1)\n",
    "    auc = plot_auc(fpr, tpr)\n",
    "    plot_calibration(y, y_hat)\n",
    "    #plt.hist(y_hat[y_hat >= 0], bins=10, density=False)\n",
    "    plt.hist(y_hat, bins=100, density=True)\n",
    "    plt.title(\"Histogram of predicted probabilities for click\")\n",
    "    plt.show()\n",
    "   \n",
    "    result = {\n",
    "        \"Accuracy\": accuracy_score(y, y_pred, normalize=True),\n",
    "        \"Recall\": recall_score(y, y_pred),\n",
    "        \"Precision\": precision_score(y, y_pred),\n",
    "        \"AUC\": auc,\n",
    "        \"-LogLoss(higer is better)\": -log_loss(y, y_pred),\n",
    "        \"threshold\":   best_threshold,\n",
    "        \"params\": json.dumps(params)\n",
    "    }\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity  = FunctionTransformer(lambda x:x, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Random Forest PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rf_pipe_line(calibration = None, params = None , target_encoding = True):\n",
    "    assert calibration is None or calibration in ['isotonic', 'sigmoid']\n",
    "    encoder = ce.TargetEncoder(cols=cat_features) if target_encoding else identity\n",
    "    rf = RandomForestClassifier(n_estimators = 200, max_depth=5, criterion = \"gini\", n_jobs = 4,\n",
    "                                min_samples_split=100,random_state=13)\n",
    "    if params:\n",
    "        rf.set_params(**params)\n",
    "    if calibration is None:\n",
    "        rf_pipe = Pipeline([('target_enc',encoder),  ('classifier', rf)])\n",
    "    else:\n",
    "        calibrated_RF_classifier =  CalibratedClassifierCV(base_estimator=rf, cv=5, method=calibration)\n",
    "        rf_pipe = Pipeline([('target_enc',encoder),  ('calibrated_classifier', calibrated_RF_classifier)])\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    return rf_pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters being tuned:\n",
    "1. min_samples_leaf = the minimal number of observations in a new leaf that are required in order for creating it\n",
    "2. max_depth        = the maximal depth that is allowed in each tree (# splits a in any directed path)\n",
    "3. class_weight     = the strategy for creating class weights to handle imbalanced data\n",
    "4. n_estimators     = the totl number of trees in the forest\n",
    "5. criterion        = the metric for calculating improvement in information by the node splitting: Gini inequality or entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a long step, was executed and saved previously\n",
    "\n",
    "if CALC_PARAMS:\n",
    "\n",
    "    if dev_mode:\n",
    "        rf_params_grid ={'min_samples_leaf': [50, 100], \n",
    "                     'max_depth': [3,5] ,\n",
    "                     'class_weight' : ['balanced'],\n",
    "                     'n_estimators': [100],\n",
    "                     'criterion': [\"gini\"]\n",
    "\n",
    "                    }     \n",
    "    else:    \n",
    "        rf_params_grid ={'min_samples_leaf': [50, 100, 500], \n",
    "                         'max_depth': [3,5] ,\n",
    "                         'class_weight' : ['balanced', None],\n",
    "                         'n_estimators': [100, 200],\n",
    "                         'criterion': [\"gini\"]\n",
    "\n",
    "                        }\n",
    "    best_rf_params = find_best_params_random_forest(rf_params_grid, X_train, y_train, X_val, y_val)\n",
    "else:\n",
    "    get_config\n",
    "    best_rf_params = get_config('RF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_pipe = make_rf_pipe_line(calibration = 'sigmoid' if USE_SMOTE else None,\n",
    "                            params = best_rf_params,\n",
    "                            target_encoding = (USE_SMOTE==False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = evaulate_pipeline(rf_pipe, X_val, y_val,best_rf_params)\n",
    "rf_df = pd.DataFrame(rf_results, index= ['RF'])\n",
    "rf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = dev_val[features]\n",
    "y_val = dev_val['clicked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_pool = Pool(data = X_val, label = y_val, cat_features=cat_features)\n",
    "train_pool = Pool(data = X_train, label = y_train, cat_features=cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catboost_pipe_line(train_pool, validation_pool , class_weight = None, target_encoder = False, params = None):\n",
    "    model = CatBoostClassifier(iterations=500,\n",
    "                               learning_rate=None,\n",
    "                              cat_features= cat_features ,\n",
    "                               depth=5,\n",
    "                               custom_loss = ['AUC', 'Logloss','BrierScore', 'Precision', 'Recall'],\n",
    "                               early_stopping_rounds = 20,\n",
    "                               auto_class_weights = class_weight,\n",
    "                               subsample= 0.5,\n",
    "                               verbose=30)\n",
    "    if params:\n",
    "        model.set_params(**params)\n",
    "    model.fit(train_pool, eval_set=validation_pool, plot=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters being tuned:\n",
    "1. min_child_samples  = the minimal number of observations in a new leaf that are required in order for creating it\n",
    "2. depth              = the maximal depth that is allowed in each tree (# splits a in any directed path)\n",
    "3. l2_leaf_reg        = L2 regularization coeffecient  \n",
    "4. n_estimators       = the totl number of trees in the forest\n",
    "5. subsample          = proprtion of rows sampled in each tree growin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dev_mode:\n",
    "    catboost_params_grid ={ \n",
    "                      'depth': [4,6,8] ,\n",
    "                      'min_child_samples' : [100,500],\n",
    "                      'n_estimators': [100],\n",
    "                      'subsample': [0.95],\n",
    "                      'l2_leaf_reg':[1],\n",
    "\n",
    "                    }\n",
    "else:     \n",
    "    catboost_params_grid ={ \n",
    "                      'depth': [4,6,8] ,\n",
    "                      'min_child_samples' : [100,500,1000],\n",
    "                      'n_estimators': [100,200],\n",
    "                      'subsample': [0.75,0.95],\n",
    "                      'l2_leaf_reg':[1,2],\n",
    "\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=500,\n",
    "                               learning_rate=None,\n",
    "                              cat_features=cat_features,\n",
    "                               depth=5,\n",
    "                               early_stopping_rounds = 20,\n",
    "                               auto_class_weights = 'Balanced',\n",
    "                               subsample= 0.5,\n",
    "                               verbose=30)\n",
    "if CALC_PARAMS:\n",
    "\n",
    "    search_results = model.randomized_search(catboost_params_grid,\n",
    "                      train_pool,\n",
    "                      y=None,\n",
    "                      cv=3,\n",
    "                      n_iter=10,\n",
    "                      partition_random_seed=0,\n",
    "                      calc_cv_statistics=True,\n",
    "                      search_by_train_test_split=True,\n",
    "                      refit=True,\n",
    "                      shuffle=True,\n",
    "                      stratified=None,\n",
    "                      train_size=0.8,\n",
    "                      verbose=False,\n",
    "                      log_cout=sys.stdout,\n",
    "                  log_cerr=sys.stderr)\n",
    "\n",
    "    best_catboost_params = search_results['params']\n",
    "    best_catboost_params\n",
    "\n",
    "else:\n",
    "    best_catboost_params = get_config('catboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimized configuration for catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_catboost_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a final cat boost model with the optimized params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_pipe = make_catboost_pipe_line(train_pool, validation_pool, target_encoder = False, params = best_catboost_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_results = evaulate_pipeline(catboost_pipe, X_val, y_val, best_catboost_params)\n",
    "catboost_df = pd.DataFrame(catboost_results, index= ['catboost'])\n",
    "catboost_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: KNN Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_knn_pipe_line(params = None) :\n",
    "    encoder = ce.TargetEncoder(cols=cat_features)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=300, weights='uniform')\n",
    "    if params:\n",
    "        knn_model.set_params(**params)\n",
    "    return (Pipeline([('target_enc',encoder),  ('scaler',  StandardScaler()), ('KNN', knn_model)]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_knn(grid,X_train, y_train, X_val, y_val):\n",
    "    best_accuracy = float('-inf')\n",
    "    worst_accuracy = float('inf')\n",
    "    for g in ParameterGrid(grid):\n",
    "        print(g)\n",
    "        model = make_knn_pipe_line(params = g)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_hat = model.predict_proba(X_val)[:,1]\n",
    "        curr_accuracy, best_threshold = find_threshold_by_accuracy(y_val, y_hat, plot=False)\n",
    "\n",
    "        # save if best\n",
    "        if curr_accuracy > best_accuracy:\n",
    "            best_accuracy = curr_accuracy\n",
    "            best_grid = g\n",
    "            \n",
    "        if curr_accuracy < worst_accuracy:\n",
    "            worst_accuracy = curr_accuracy\n",
    "            worst_grid = g\n",
    "    print (\"Final Result\")\n",
    "    print (\"best accuracy\", best_accuracy, \", Grid:\", best_grid)\n",
    "    print (\"worst accuracy\", worst_accuracy, \", Grid:\", worst_grid)\n",
    "    return best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters being tuned:\n",
    "1. n_neighbors        = number of nearset neighbouring observation to consider for deciding on the prediciton\n",
    "2. weights       = should all neighbours have equal weight or weighted by the distance to the observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Note: previous runs showed that uniform weighting is consistently better than distanced based weighting\n",
    "# hence due to slow perfromance of this classifier, we avoid the distance based weighting option\n",
    "if CALC_PARAMS:\n",
    "    if dev_mode:\n",
    "        knn_params_grid ={'n_neighbors': [500], \n",
    "                         'weights': ['uniform']\n",
    "                        }\n",
    "    else:\n",
    "        knn_params_grid ={'n_neighbors': [500, 1000, 1500], \n",
    "                 'weights': ['uniform']\n",
    "                }\n",
    "    best_knn_params = find_best_params_knn(knn_params_grid, X_train, y_train, X_val, y_val)\n",
    "    best_knn_params\n",
    "else:\n",
    "    best_knn_params = get_config('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = make_knn_pipe_line(params = best_knn_params)\n",
    "knn_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = evaulate_pipeline(knn_pipeline, X_val, y_val, best_knn_params);\n",
    "knn_df = pd.DataFrame(knn_results, index= ['knn'])\n",
    "knn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary of model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = pd.concat([rf_df, knn_df,catboost_df])\n",
    "metrics_df.sort_values(by='AUC')\n",
    "\n",
    "if CALC_PARAMS:\n",
    "    now = datetime.datetime.now()\n",
    "    now_string = now.strftime(\"%m_%d_%Y__%H_%M\")\n",
    "    metrics_df.reset_index().to_csv(f\"configuration_{now_string}_sample_{SAMPLE}.csv\", index=False)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exaplinable AI:  Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost_pipe\n",
    "\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap Values (Shapely Additive exPlanations) is a model Agnostic method for explaining model predictions\n",
    "It is based on theory developed by Loid Shapley in the 1950's \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explanation\n",
    "This graph is actually a series of dot-plots:every observation(row) is representated as a dot\n",
    "for each variable in the model we have a dot-plot showing a dot for each observation in the train set\n",
    "X-axis: The shap value i.e. the negative or positive contribution of the observation's variable to the final prediction\n",
    "Y-axis: in case where many observation have the same shap value the dots are stacked vertically\n",
    "Color: For continuous variables the color represent whether the value of the variable is high or low in the observation\n",
    "\n",
    "##### Example: TODO verify\n",
    "in the device_diag variable we see that large screens (strong red) are associated with increased (positive) probability for a click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_simple(idx, X_train):\n",
    "    data = X_train.iloc[[idx]]\n",
    "    shap_values = explainer.shap_values( data)\n",
    "    return (shap.force_plot(explainer.expected_value,shap_values , data, link='logit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 random predictions local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_simple(random.randint(1, X_train.shape[0]), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_simple(random.randint(1, X_train.shape[0]), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_simple(random.randint(1, X_train.shape[0]), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Prediction on external data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv(\"ctr_dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_test, _ = preprocessing_pipeline(testset, one_hot_encoding=USE_SMOTE, allowed_levels_dict = allowed_levels_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add empty dummy columns for levels that are missing in the predicitons data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_COLS.remove('clicked')\n",
    "unknown_cols = [c for c in MODEL_COLS  if c not in X_test.columns ]\n",
    "for c in unknown_cols:\n",
    "    print(f\"adding default column for {c}\")\n",
    "    X_test.loc[:, c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([c for c in X_test.columns if c not in MODEL_COLS])\n",
    "X_test = X_test[MODEL_COLS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold_catboost = catboost_df['threshold'].values[0]\n",
    "best_threshold_catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_hat[:,1] >= best_threshold_catboost).astype(int)\n",
    "predictions = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total predictions , total clicks predicted,  Percentage of predicted clicks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"N={predictions.count()}, Clicks={predictions.sum()}, Clicks Percent= {round(predictions.mean()*100,3)}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 8 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('output_8.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
